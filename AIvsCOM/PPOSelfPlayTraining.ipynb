{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameId = \"doapp\"\n",
    "#gameId = \"sfiii3n\"\n",
    "#gameId = \"tektagt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "# Algorithm options\n",
    "algoOpt = namedtuple('algoOptions', 'character frameSize frameStack actionSpace attackButComb\\\n",
    "                                     frameRatio actionBufLen dilation smallCnn')\n",
    "if gameId == \"doapp\":\n",
    "    algoOpt.character = \"Random\"\n",
    "elif gameId == \"sfiii3n\":\n",
    "    algoOpt.character = \"Random\"\n",
    "elif gameId == \"tektagt\":\n",
    "    algoOpt.character = [\"Random\", \"Random\"]\n",
    "\n",
    "algoOpt.frameRatio    = 6\n",
    "algoOpt.frameSize     = 128\n",
    "algoOpt.frameStack    = 4\n",
    "algoOpt.dilation      = 1\n",
    "algoOpt.actionSpace   = \"discrete\"\n",
    "algoOpt.attackButComb = True\n",
    "algoOpt.actionBufLen  = 12\n",
    "algoOpt.smallCnn      = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "timeDepSeed = int((time.time()-int(time.time()-0.5))*1000)\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../'))\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../../gym/'))\n",
    "\n",
    "specificAlgoOptId = str(\"\".join(algoOpt.character)) + \"x\" + str(algoOpt.frameRatio) + \"_\" +\\\n",
    "                    str(algoOpt.frameSize) + \"x\" + str(algoOpt.frameStack) + \"x\" +\\\n",
    "                    str(algoOpt.dilation) + \"x\" + str(algoOpt.smallCnn) + \"_\" +\\\n",
    "                    algoOpt.actionSpace + \"x\" + str(algoOpt.attackButComb) + \"x\" + str(algoOpt.actionBufLen)\n",
    "                    \n",
    "tensorBoardFolder = \"./{}_ppo2_selfPlay_TB_{}/\".format(gameId, specificAlgoOptId)\n",
    "modelFolder = \"./{}_ppo2_selfPlay_Model_{}\".format(gameId, specificAlgoOptId)\n",
    "\n",
    "os.makedirs(modelFolder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from makeStableBaselinesEnv import makeStableBaselinesEnv\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sbUtils import linear_schedule, AutoSave, UpdateRLPolicyWeights\n",
    "from customPolicies.customCnnPolicy import CustCnnPolicy, local_nature_cnn_small\n",
    "from policies import noActionPolicy, randomPolicy, RLPolicy\n",
    "\n",
    "from stable_baselines import PPO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diambra environment kwargs\n",
    "diambraKwargs = {}\n",
    "diambraKwargs[\"romsPath\"]   = \"../../roms/mame/\"\n",
    "diambraKwargs[\"binaryPath\"] = \"../../customMAME/\"\n",
    "diambraKwargs[\"frameRatio\"] = algoOpt.frameRatio\n",
    "diambraKwargs[\"render\"]     = True\n",
    "\n",
    "diambraKwargs[\"player\"] = \"P1P2\" # 2P game   \n",
    "\n",
    "# Game dependent kwawrgs    \n",
    "diambraKwargs[\"characters\"] =[algoOpt.character, algoOpt.character]    \n",
    "diambraKwargs[\"charOutfits\"] =[2, 2]\n",
    "\n",
    "# DIAMBRA gym kwargs\n",
    "diambraGymKwargs = {}\n",
    "diambraGymKwargs[\"actionSpace\"] = [algoOpt.actionSpace, algoOpt.actionSpace]\n",
    "diambraGymKwargs[\"attackButCombinations\"] = [algoOpt.attackButComb, algoOpt.attackButComb]\n",
    "\n",
    "# Gym Wrappers kwargs\n",
    "wrapperKwargs = {}\n",
    "wrapperKwargs[\"noOpMax\"] = 0\n",
    "wrapperKwargs[\"hwcObsResize\"] = [algoOpt.frameSize, algoOpt.frameSize, 1]\n",
    "wrapperKwargs[\"normalizeRewards\"] = True\n",
    "wrapperKwargs[\"clipRewards\"] = False\n",
    "wrapperKwargs[\"frameStack\"] = algoOpt.frameStack\n",
    "wrapperKwargs[\"dilation\"] = algoOpt.dilation\n",
    "wrapperKwargs[\"scale\"] = True\n",
    "wrapperKwargs[\"scaleMod\"] = 0\n",
    "\n",
    "# Additional observations\n",
    "keyToAdd = []\n",
    "keyToAdd.append(\"actionsBuf\") # env.actBufLen*(env.n_actions[0]+env.n_actions[1])\n",
    "\n",
    "if gameId != \"tektagt\":                                                         \n",
    "    keyToAdd.append(\"ownHealth\")   # 1                                            \n",
    "    keyToAdd.append(\"oppHealth\")   # 1                                                \n",
    "else:                                                                           \n",
    "    keyToAdd.append(\"ownHealth1\") # 1                                             \n",
    "    keyToAdd.append(\"ownHealth2\") # 1                                             \n",
    "    keyToAdd.append(\"oppHealth1\") # 1                                              \n",
    "    keyToAdd.append(\"oppHealth2\") # 1  \n",
    "    keyToAdd.append(\"ownActiveChar\") # 1\n",
    "    keyToAdd.append(\"oppActiveChar\") # 1\n",
    "    \n",
    "keyToAdd.append(\"ownPosition\")     # 1\n",
    "keyToAdd.append(\"oppPosition\")     # 1\n",
    "keyToAdd.append(\"ownChar\")       # len(env.charNames)\n",
    "keyToAdd.append(\"oppChar\")       # len(env.charNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gameId == \"doapp\":\n",
    "    nActions = [9, 8] if algoOpt.attackButComb else [9, 4] \n",
    "else:\n",
    "    raise ValueError(\"nActions not provided for selected gameId = {}\".format(gameId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noAction_policy = noActionPolicy(actionSpace=\"discrete\")\n",
    "random_policy = randomPolicy(nActions, actionSpace=\"discrete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2.load(os.path.join(modelFolder, str(\"_\".join(keyToAdd))+\"_0M\"))\n",
    "\n",
    "deterministicFlag = False                                                    \n",
    "rl_policy = RLPolicy(model, deterministicFlag, nActions, name=\"PPO-0M\", \n",
    "                     actionSpace=diambraGymKwargs[\"actionSpace\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEnv=1\n",
    "\n",
    "envId = gameId + \"_Train\"\n",
    "env = makeStableBaselinesEnv(envId, numEnv, timeDepSeed, diambraKwargs, diambraGymKwargs,\n",
    "                             wrapperKwargs, keyToAdd=keyToAdd, p2Mode=\"selfPlayVsRL\", \n",
    "                             p2Policy=rl_policy, useSubprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Obs_space = \", env.observation_space)\n",
    "print(\"Obs_space type = \", env.observation_space.dtype)\n",
    "print(\"Obs_space high = \", env.observation_space.high)\n",
    "print(\"Obs_space low = \", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Act_space = \", env.action_space)\n",
    "print(\"Act_space type = \", env.action_space.dtype)\n",
    "if diambraGymKwargs[\"actionSpace\"][0] == \"multiDiscrete\":\n",
    "    print(\"Act_space n = \", env.action_space.nvec)\n",
    "else:\n",
    "    print(\"Act_space n = \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy param\n",
    "if nActions != env.get_attr(\"nActions\")[0][0]:\n",
    "    raise ValueError(\"nActions values wrong\")\n",
    "nActions = env.get_attr(\"nActions\")[0][0]\n",
    "actBufLen = env.get_attr(\"actBufLen\")[0]\n",
    "nChar = env.get_attr(\"numberOfCharacters\")[0]\n",
    "\n",
    "policyKwargs={}\n",
    "policyKwargs[\"n_add_info\"] = actBufLen*(nActions[0]+nActions[1]) + len(keyToAdd)-3 + 2*nChar\n",
    "policyKwargs[\"layers\"] = [64, 64]\n",
    "\n",
    "if algoOpt.smallCnn:\n",
    "    policyKwargs[\"cnn_extractor\"] = local_nature_cnn_small\n",
    "\n",
    "print(\"nActions =\", nActions)\n",
    "print(\"nChar =\", nChar)\n",
    "print(\"n_add_info =\", policyKwargs[\"n_add_info\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO param\n",
    "setGamma = 0.94\n",
    "setLearningRate = linear_schedule(2.5e-4, 2.5e-6)\n",
    "setClipRange = linear_schedule(0.15, 0.025)\n",
    "setClipRangeVf = setClipRange\n",
    "\n",
    "# Initialize the model\n",
    "model = PPO2(CustCnnPolicy, env, verbose=1, \n",
    "             gamma = setGamma, nminibatches=4, noptepochs=4, n_steps=128,\n",
    "             learning_rate=setLearningRate, cliprange=setClipRange, cliprange_vf=setClipRangeVf, \n",
    "             tensorboard_log=tensorBoardFolder, policy_kwargs=policyKwargs)\n",
    "\n",
    "#OR\n",
    "\n",
    "# Load the trained agent\n",
    "#model = PPO2.load(os.path.join(modelFolder, str(\"_\".join(keyToAdd))+\"_0M\"), env=env, tensorboard_log=tensorBoardFolder, \n",
    "#                  policy_kwargs=policyKwargs, gamma = setGamma, learning_rate=setLearningRate, \n",
    "#                  cliprange=setClipRange, cliprange_vf=setClipRangeVf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model discount factor = \", model.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the callback: autosave every USER DEF steps\n",
    "autoSaveCallback = AutoSave(check_freq=1000000, numEnv=numEnv,\n",
    "                            save_path=os.path.join(modelFolder, str(\"_\".join(keyToAdd))+\"_0M_\"))\n",
    "\n",
    "prevAgentsSamplingDict = {\"probability\": 0.3,\n",
    "                          \"list\":[os.path.join(modelFolder, str(\"_\".join(keyToAdd))+\"_0M\")]}\n",
    "upRLPolWCallback = UpdateRLPolicyWeights(check_freq=200000, numEnv=numEnv, save_path=modelFolder,\n",
    "                                         prevAgentsSampling=prevAgentsSamplingDict)\n",
    "\n",
    "# Train the agent\n",
    "timeSteps = 10000000\n",
    "model.learn(total_timesteps=timeSteps, callback=[autoSaveCallback, upRLPolWCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(os.path.join(modelFolder, str(\"_\".join(keyToAdd))+\"_0M\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
