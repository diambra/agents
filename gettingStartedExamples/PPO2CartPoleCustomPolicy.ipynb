{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.bench import Monitor\n",
    "\n",
    "from stable_baselines.common.policies import ActorCriticPolicy, register_policy, mlp_extractor, MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.tf_layers import linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MLP policy of three layers of size 128 each for the actor and 2 layers of 32 for the critic,\n",
    "# with a MLP feature extractor\n",
    "class CustomPolicyNew(ActorCriticPolicy):\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **kwargs):\n",
    "        super(CustomPolicyNew, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, \n",
    "                                           reuse=reuse, scale=False)\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            activ = tf.nn.relu\n",
    "\n",
    "            pi_h, vf_h = mlp_extractor(tf.layers.flatten(self.processed_obs), \n",
    "                                               net_arch=[dict(vf=[64, 64], pi=[64, 64])],\n",
    "                                               act_fun=tf.tanh, **kwargs)\n",
    "            #extracted_features = tf.layers.flatten(extracted_features)\n",
    "\n",
    "            #pi_h = extracted_features\n",
    "            #for i, layer_size in enumerate([128, 128, 128]):\n",
    "            #    pi_h = activ(tf.layers.dense(pi_h, layer_size, name='pi_fc' + str(i)))\n",
    "            pi_latent = pi_h\n",
    "\n",
    "            #vf_h = extracted_features\n",
    "            #for i, layer_size in enumerate([32, 32]):\n",
    "            #    vf_h = activ(tf.layers.dense(vf_h, layer_size, name='vf_fc' + str(i)))\n",
    "            value_fn = tf.layers.dense(vf_h, 1, name='vf')\n",
    "            vf_latent = vf_h\n",
    "\n",
    "            self._proba_distribution, self._policy, self.q_value = \\\n",
    "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
    "\n",
    "        self._value_fn = value_fn\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
    "        if deterministic:\n",
    "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        else:\n",
    "            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        return action, value, self.initial_state, neglogp\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
    "\n",
    "    def value(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.value_flat, {self.obs_ph: obs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference implementation for FeedForward model already included in baselines\n",
    "class CustomPolicy(ActorCriticPolicy):\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, layers=None, \n",
    "                 net_arch=None, act_fun=tf.tanh, feature_extraction=\"mlp\", **kwargs):\n",
    "        super(CustomPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse,\n",
    "                                                scale=(feature_extraction == \"cnn\"))\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "\n",
    "        if layers is not None:\n",
    "            warnings.warn(\"Usage of the `layers` parameter is deprecated! Use net_arch instead \"\n",
    "                          \"(it has a different semantics though).\", DeprecationWarning)\n",
    "            if net_arch is not None:\n",
    "                warnings.warn(\"The new `net_arch` parameter overrides the deprecated `layers` parameter!\",\n",
    "                              DeprecationWarning)\n",
    "\n",
    "        if net_arch is None:\n",
    "            if layers is None:\n",
    "                layers = [64, 64]\n",
    "            net_arch = [dict(vf=layers, pi=layers)]\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            pi_latent, vf_latent = mlp_extractor(tf.layers.flatten(self.processed_obs), net_arch, act_fun)\n",
    "\n",
    "            self._value_fn = linear(vf_latent, 'vf', 1)\n",
    "\n",
    "            self._proba_distribution, self._policy, self.q_value = \\\n",
    "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
    "\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
    "        if deterministic:\n",
    "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        else:\n",
    "            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        return action, value, self.initial_state, neglogp\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
    "\n",
    "    def value(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.value_flat, {self.obs_ph: obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "model = PPO2(CustomPolicyNew, env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "time_steps = 50000\n",
    "model.learn(total_timesteps=time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enjoy trained agent\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
