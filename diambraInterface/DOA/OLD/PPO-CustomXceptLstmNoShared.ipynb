{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameFolder = \"DOA++-MAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import time\n",
    "timeDepSeed = int((time.time()-int(time.time()-0.5))*1000)\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../'))   \n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../../../games',gameFolder))   \n",
    "\n",
    "tensorBoardFolder = \"./ppo2_TB_CustXceptLstmNoShared/\"\n",
    "modelFolder = \"ppo2_Model_CustXceptLstmNoShared/\"\n",
    "\n",
    "os.makedirs(modelFolder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from makeDiambraEnv import *\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from customPolicies.customXceptLstmPolicyNoShared import *\n",
    "\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "from stable_baselines.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoSave(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param save_path: (str) Path to the folder where the model will be saved.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, save_path: str, verbose=1):\n",
    "        super(AutoSave, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path_base = save_path + 'autoSave_'\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Example for saving best model\n",
    "            if self.verbose > 0:\n",
    "                print(\"Saving new best model to {}\".format(self.save_path_base))\n",
    "            # Save the agent\n",
    "            self.model.save(self.save_path_base+str(self.n_calls)+\"steps_action+_add\")\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diambraKwargs = {}\n",
    "diambraKwargs[\"roms_path\"] = \"../../../roms/MAMEToolkit/roms/\"\n",
    "diambraKwargs[\"binary_path\"] = \"../../../../customMAME/\"\n",
    "diambraKwargs[\"player\"] = \"P1\"\n",
    "diambraKwargs[\"frame_ratio\"] = 3\n",
    "diambraKwargs[\"render\"] = False\n",
    "#diambraKwargs[\"throttle\"] = False\n",
    "#diambraKwargs[\"sound\"] = False \n",
    "#diambraKwargs[\"character\"] =\"Random\"\n",
    "diambraKwargs[\"character\"] = \"Kasumi\"\n",
    "\n",
    "wrapperKwargs = {}\n",
    "wrapperKwargs[\"frame_stack\"] = 1\n",
    "wrapperKwargs[\"clip_rewards\"] = False\n",
    "wrapperKwargs[\"normalize_rewards\"] = True\n",
    "wrapperKwargs[\"scale\"] = True\n",
    "wrapperKwargs[\"scale_mod\"] = -1\n",
    "wrapperKwargs[\"hwc_obs_resize\"] = [256, 256, 1]\n",
    "\n",
    "#keyToAdd = None\n",
    "keyToAdd = []\n",
    "keyToAdd.append(\"actionsBuf\")\n",
    "#keyToAdd.append(\"player\")\n",
    "keyToAdd.append(\"healthP1\")\n",
    "keyToAdd.append(\"healthP2\")\n",
    "keyToAdd.append(\"positionP1\")\n",
    "keyToAdd.append(\"positionP2\")\n",
    "#keyToAdd.append(\"winsP1\")\n",
    "#keyToAdd.append(\"winsP2\")\n",
    "\n",
    "numEnv=1\n",
    "\n",
    "env = make_diambra_env(diambraMame, env_prefix=\"Train\", num_env=numEnv, seed=timeDepSeed, \n",
    "                       continue_game = 0, diambra_kwargs = diambraKwargs, \n",
    "                       wrapper_kwargs = wrapperKwargs, key_to_add = keyToAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Obs_space = \", env.observation_space)\n",
    "print(\"Obs_space type = \", env.observation_space.dtype)\n",
    "print(\"Obs_space high = \", env.observation_space.high)\n",
    "print(\"Obs_space low = \", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Act_space = \", env.action_space)\n",
    "print(\"Act_space type = \", env.action_space.dtype)\n",
    "print(\"Act_space n = \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy arguments\n",
    "policyKwargs={}\n",
    "# Frame observation\n",
    "policyKwargs[\"scale_in\"] = False\n",
    "policyKwargs[\"n_lstm\"] = 256\n",
    "policyKwargs[\"cnn_input\"] = wrapperKwargs[\"hwc_obs_resize\"] # [256, 256, 1]\n",
    "policyKwargs[\"cnn_embeddings\"] = 512\n",
    "\n",
    "# Additional info observation\n",
    "policyKwargs[\"n_add_info\"] = 148\n",
    "policyKwargs[\"layers\"] = [128, 64]\n",
    "\n",
    "# Policy and value net part\n",
    "policyKwargs[\"layers_policy\"] = [64, 64]\n",
    "policyKwargs[\"layers_value\"] = [64, 64]\n",
    "\n",
    "# Model arguments to override load\n",
    "#customObjects={}\n",
    "#customObjects[\"learning_rate\"]=5e-5 # Changed from 2.5e-4 to 5e-5 @ ~6.6M iter\n",
    "\n",
    "# Initialize the model, 1 env\n",
    "model = PPO2(CustomXceptLstmPolicyNoShared, env, nminibatches=numEnv, verbose=1, n_steps=32,\n",
    "             learning_rate=2.5e-4, tensorboard_log=tensorBoardFolder, gamma = 0.94, \n",
    "             policy_kwargs=policyKwargs)\n",
    "\n",
    "#OR\n",
    "\n",
    "# Load the trained agent, 1 env\n",
    "#model = PPO2.load(modelFolder+\"7_3Msteps_action+_add\", env=env, custom_objects=customObjects,\n",
    "#                  tensorboard_log=tensorBoardFolder, \n",
    "#                  policy_kwargs=policyKwargs, gamma = 0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the callback: autosave every 100000 steps\n",
    "autoSaveCallback = AutoSave(check_freq=100000, save_path=modelFolder+\"start_\")\n",
    "\n",
    "# Train the agent\n",
    "time_steps = 10000000\n",
    "model.learn(total_timesteps=time_steps, callback=autoSaveCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(modelFolder+\"6_6Msteps_action+_add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "states = None\n",
    "\n",
    "cumulativeEpRew = 0.0\n",
    "cumulativeEpRewAll = []\n",
    "cumulativeTotRew = 0.0\n",
    "\n",
    "maxNumEp = 100\n",
    "currNumEp = 0\n",
    "\n",
    "while currNumEp < maxNumEp:\n",
    "\n",
    "    action, states = model.predict(observation, states, deterministic=False)\n",
    "    action_prob = model.action_probability(observation, states)\n",
    "    #print(\"Action probabilities = \", action_prob)\n",
    "    #print(\"Max action = \", np.argmax(action_prob))\n",
    "    #print(\"Action = \", action)\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    cumulativeEpRew += reward\n",
    "    \n",
    "    if np.any(done):\n",
    "        currNumEp += 1\n",
    "        print(\"Ep. # = \", currNumEp)\n",
    "        print(\"Ep. Cumulative Rew # = \", cumulativeEpRew)\n",
    "        sys.stdout.flush()\n",
    "        cumulativeEpRewAll.append(cumulativeEpRew)\n",
    "        cumulativeTotRew += cumulativeEpRew\n",
    "        cumulativeEpRew = 0.0\n",
    "\n",
    "        observation = env.reset()\n",
    "        states = None\n",
    "\n",
    "print(\"Mean cumulative reward = \", cumulativeTotRew/maxNumEp)    \n",
    "print(\"Mean cumulative reward = \", np.mean(cumulativeEpRewAll))    \n",
    "print(\"Std cumulative reward = \", np.std(cumulativeEpRewAll))   \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "states = None\n",
    "\n",
    "while True:\n",
    "\n",
    "    action, states = model.predict(observation, states, deterministic=False)\n",
    "    action_prob = model.action_probability(observation, states)\n",
    "    #print(\"Action probabilities = \", action_prob)\n",
    "    #print(\"Max action = \", np.argmax(action_prob))\n",
    "    #print(\"Action = \", action)\n",
    "    #input(\"Pausa\")\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "        states = None\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
