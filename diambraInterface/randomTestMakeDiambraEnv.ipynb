{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameFolder = \"DOA++-MAME\"\n",
    "#gameFolder = \"SFIII-MAME\"\n",
    "#gameFolder = \"UMK3-MAME\"\n",
    "#gameFolder = \"TEKTAG-MAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import time\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "timeDepSeed = int((time.time()-int(time.time()-0.5))*1000)\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../../games',gameFolder))   \n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../../utils'))\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../../pythonGamePadInterface'))\n",
    "                                                                                                                                 \n",
    "from diambraGamepad import diambraGamepad \n",
    "from policies import gamepadPolicy, RLPolicy # To train AI against another AI or HUM\n",
    "from makeDiambraEnv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common settings\n",
    "\n",
    "diambraKwargs = {}\n",
    "diambraKwargs[\"roms_path\"] = \"../../roms/MAMEToolkit/roms/\"\n",
    "diambraKwargs[\"binary_path\"] = \"../../../customMAME/\"\n",
    "diambraKwargs[\"frame_ratio\"] = 6\n",
    "\n",
    "\n",
    "diambraKwargs[\"player\"] = \"P1\" # 1P\n",
    "#diambraKwargs[\"player\"] = \"P1P2\" # 2P\n",
    "\n",
    "#keyToAdd = None\n",
    "keyToAdd = []\n",
    "keyToAdd.append(\"actionsBufP1\")\n",
    "if diambraKwargs[\"player\"] == \"P1P2\":\n",
    "    keyToAdd.append(\"actionsBufP2\") # Only 2P\n",
    "\n",
    "if gameFolder != \"TEKTAG-MAME\": # DOA, SFIII, UMK3\n",
    "    keyToAdd.append(\"ownHealth\")\n",
    "    keyToAdd.append(\"oppHealth\")\n",
    "else: # TEKTAG\n",
    "    keyToAdd.append(\"ownHealth_1\")\n",
    "    keyToAdd.append(\"ownHealth_2\")\n",
    "    keyToAdd.append(\"oppHealth_1\")\n",
    "    keyToAdd.append(\"oppHealth_2\")\n",
    "\n",
    "keyToAdd.append(\"ownPosition\")\n",
    "keyToAdd.append(\"oppPosition\")\n",
    "#keyToAdd.append(\"ownWins\")\n",
    "#keyToAdd.append(\"oppWins\")\n",
    "#keyToAdd.append(\"stage\")\n",
    "keyToAdd.append(\"characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOA\n",
    "diambraKwargs[\"characters\"] =[\"Raidou\", \"Kasumi\"]\n",
    "diambraKwargs[\"charOutfits\"] =[4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GamePad policy initialization\n",
    "gamePad_policy = gamepadPolicy(diambraGamepad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAMBRA gym kwargs\n",
    "diambraGymKwargs = {}\n",
    "diambraGymKwargs[\"P2brain\"] = None#gamePad_policy\n",
    "diambraGymKwargs[\"continue_game\"] = 0.0\n",
    "diambraGymKwargs[\"show_final\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapperKwargs = {}\n",
    "wrapperKwargs[\"hwc_obs_resize\"] = [256, 256, 1]\n",
    "wrapperKwargs[\"normalize_rewards\"] = True\n",
    "wrapperKwargs[\"clip_rewards\"] = False\n",
    "wrapperKwargs[\"frame_stack\"] = 6\n",
    "wrapperKwargs[\"dilation\"] = 1\n",
    "wrapperKwargs[\"scale\"] = True\n",
    "wrapperKwargs[\"scale_mod\"] = 0\n",
    "\n",
    "numEnv=1\n",
    "\n",
    "env = make_diambra_env(diambraMame, env_prefix=\"Test\", num_env=numEnv, seed=timeDepSeed, \n",
    "                       diambra_kwargs=diambraKwargs, diambra_gym_kwargs=diambraGymKwargs,\n",
    "                       wrapper_kwargs=wrapperKwargs, key_to_add=keyToAdd, no_vec=True,\n",
    "                       rec_file_path=\"./RLTraj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Obs space =\", env.observation_space)\n",
    "print(\"Obs space type =\", env.observation_space.dtype)\n",
    "print(\"Obs space high bound =\", env.observation_space.high)\n",
    "print(\"Obs space low bound =\", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "shp = observation.shape\n",
    "\n",
    "additionalPar = int(observation[0,0,shp[2]-1])\n",
    "\n",
    "nScalarAddPar = additionalPar - 2*len(env.charNames) - env.actBufLen*(env.n_actions[0]+env.n_actions[1]) # 1P\n",
    "if diambraKwargs[\"player\"] == \"P1P2\": \n",
    "    nScalarAddPar = additionalPar - 2*len(env.charNames) - 2*env.actBufLen*(env.n_actions[0]+env.n_actions[1])# 2P\n",
    "\n",
    "print(\"Additional Par = \", additionalPar)\n",
    "print(\"N scalar actions = \", nScalarAddPar)\n",
    "#input(\"Pause\")\n",
    "\n",
    "cumulativeEpRew = 0.0\n",
    "cumulativeEpRewAll = []\n",
    "\n",
    "maxNumEp = 100\n",
    "currNumEp = 0\n",
    "\n",
    "#sess = tf.Session();\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "while currNumEp < maxNumEp:\n",
    "\n",
    "    #t1 = time.time()\n",
    "    #print(\"fps = \", 1/(t1 - t0))\n",
    "    #t0 = t1\n",
    "    \n",
    "\n",
    "    # 1P\n",
    "    action = [0, 0]\n",
    "    #action = env.action_space.sample()\n",
    "    \n",
    "    # 2P\n",
    "    action2 = env.action_space.sample()\n",
    "    if diambraKwargs[\"player\"] == \"P1P2\":\n",
    "        action = np.append(action, action2)\n",
    "\n",
    "    #action = int(input(\"Action\"))\n",
    "    print(\"Action:\", action)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    #if reward != 0:\n",
    "    #    print(\"Reward =\", info[\"rewards\"])\n",
    "    #    input(\"AAA\")\n",
    "    \n",
    "    doit = True\n",
    "    if info[\"round_done\"] == True or doit:\n",
    "\n",
    "        addPar = observation[:,:,shp[2]-1]\n",
    "        addPar = np.reshape(addPar, (-1))\n",
    "        addPar = addPar[1:additionalPar+1]\n",
    "        actions = addPar[0:additionalPar-nScalarAddPar-2*env.numberOfCharacters]\n",
    "        \n",
    "        limAct = [env.actBufLen * env.n_actions[0], \n",
    "                  env.actBufLen * env.n_actions[0] + env.actBufLen * env.n_actions[1]]\n",
    "        \n",
    "        moveActionsP1   = actions[0:limAct[0]]\n",
    "        attackActionsP1 = actions[limAct[0]:limAct[1]]\n",
    "        moveActionsP1   = np.reshape(moveActionsP1, (env.actBufLen,-1))\n",
    "        attackActionsP1 = np.reshape(attackActionsP1, (env.actBufLen,-1))\n",
    "        print(\"Move actions P1 = \", moveActionsP1)\n",
    "        print(\"Attack actions P1 = \", attackActionsP1)\n",
    "        #input(\"Pausa1\")\n",
    "        \n",
    "        # 2P\n",
    "        if diambraKwargs[\"player\"] == \"P1P2\":\n",
    "            moveActionsP2   = actions[limAct[1]:limAct[1]+limAct[0]]\n",
    "            attackActionsP2 = actions[limAct[1]+limAct[0]:limAct[1]+limAct[1]]\n",
    "            moveActionsP2   = np.reshape(moveActionsP2, (env.actBufLen,-1))\n",
    "            attackActionsP2 = np.reshape(attackActionsP2, (env.actBufLen,-1))\n",
    "            print(\"Move actions P2 = \", moveActionsP2)\n",
    "            print(\"Attack actions P2 = \", attackActionsP2)\n",
    "            #input(\"Pausa1\")        \n",
    "        \n",
    "        others = addPar[additionalPar-nScalarAddPar-2*env.numberOfCharacters:]\n",
    "        print(\"ownHealth = \", others[0])\n",
    "        print(\"oppHealth = \", others[1])\n",
    "        print(\"ownPosition = \", others[2])\n",
    "        print(\"oppPosition = \", others[3])\n",
    "        #print(\"stage = \", others[4])\n",
    "        print(\"Playing Char P1 = \", env.charNames[list(others[nScalarAddPar:\n",
    "                                                              nScalarAddPar + env.numberOfCharacters]).index(1.0)])\n",
    "        \n",
    "        if diambraKwargs[\"player\"] == \"P1P2\":\n",
    "            print(\"Playing Char P2 = \", env.charNames[list(others[nScalarAddPar + env.numberOfCharacters:\n",
    "                                                                  nScalarAddPar + 2*env.numberOfCharacters]).index(1.0)])\n",
    "        \n",
    "        #input(\"Pausa1\")\n",
    "        \n",
    "        #print(np.array(observation).astype(np.float32).shape)\n",
    "        #input(\"Pausa2\")\n",
    "        #print(tf.cast(observation, tf.float32).eval(session=sess))\n",
    "   \n",
    "    \n",
    "    #if info[\"round_done\"] == True or doit:\n",
    "    #    tensor = tf.cast(observation, tf.float32)\n",
    "    #    tensor2 = tensor[:,:,shp[2]-1]\n",
    "    #    print(\"Number off additional param = \", tensor2[0,0].eval(session=sess))    \n",
    "    #    tensor2 = tf.reshape(tensor2, [-1]) \n",
    "    #    #print(\"After reshaping = \", tensor2)    \n",
    "    #    tensor2 = tensor2[1:1+additionalPar]\n",
    "    #    #print(\"Tensor=\", tensor)\n",
    "    #    tensor_actions = tf.reshape(tensor2[0:additionalPar-4], [12,-1]) \n",
    "    #    print(\"Tensor=\", tensor2[additionalPar-4:additionalPar+1].eval(session=sess))\n",
    "    #    print(\"Tensor=\", tensor_actions.eval(session=sess))\n",
    "    #    input(\"Pausa\")\n",
    "        \n",
    "    obs = np.array(observation).astype(np.float32)\n",
    "    \n",
    "    for idx in range(shp[2]-1):\n",
    "        cv2.imshow(\"image\"+str(idx), obs[:,:,idx])\n",
    "    \n",
    "    #cv2.waitKey()\n",
    "    \n",
    "    #print(\"Frames shape:\", observation.shape)\n",
    "    #print(\"Reward:\", reward)\n",
    "    #print(\"Fighting = \", info[\"fighting\"])\n",
    "    #print(\"Rewards = \", info[\"rewards\"])\n",
    "    #print(\"HealthP1 = \", info[\"healthP1\"])\n",
    "    #print(\"HealthP2 = \", info[\"healthP2\"])\n",
    "    #print(\"HealthP1_1 = \", info[\"healthP1_1\"])\n",
    "    #print(\"HealthP1_2 = \", info[\"healthP1_2\"])\n",
    "    #print(\"HealthP2_1 = \", info[\"healthP2_1\"])\n",
    "    #print(\"HealthP2_2 = \", info[\"healthP2_2\"])\n",
    "    #print(\"PositionP1 = \", info[\"positionP1\"])\n",
    "    #print(\"PositionP2 = \", info[\"positionP2\"])\n",
    "    #print(\"WinP1 = \", info[\"winsP1\"]) \n",
    "    #print(\"WinP2 = \", info[\"winsP2\"])\n",
    "\n",
    "    \n",
    "    cumulativeEpRew += reward\n",
    "    \n",
    "    if np.any(done):\n",
    "        currNumEp += 1\n",
    "        print(\"Ep. # = \", currNumEp)\n",
    "        print(\"Ep. Cumulative Rew # = \", cumulativeEpRew)\n",
    "        sys.stdout.flush()\n",
    "        cumulativeEpRewAll.append(cumulativeEpRew)\n",
    "        cumulativeEpRew = 0.0\n",
    "\n",
    "        observation = env.reset()\n",
    "        \n",
    "        addPar = observation[:,:,shp[2]-1]\n",
    "        addPar = np.reshape(addPar, (-1))\n",
    "        addPar = addPar[1:additionalPar+1]\n",
    "        others = addPar[additionalPar-5-env.numberOfCharacters:]\n",
    "        input(\"Stop\")\n",
    "\n",
    "print(\"Mean cumulative reward = \", np.mean(cumulativeEpRewAll))    \n",
    "print(\"Std cumulative reward = \", np.std(cumulativeEpRewAll))       \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
