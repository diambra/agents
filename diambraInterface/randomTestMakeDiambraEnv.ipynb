{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "gameFolder = \"DOA++-MAME\"\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../../games',gameFolder))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import cv2  # pytype:disable=import-error\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "from diambraMameGym import *\n",
    "\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "\n",
    "from stable_baselines.common.policies import CnnPolicy\n",
    "from stable_baselines import PPO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=6):\n",
    "        \"\"\"\n",
    "        Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be last action (env.n_actions[0]*env.n_actions[1] -1).\n",
    "        :param env: (Gym Environment) the environment to wrap\n",
    "        :param noop_max: (int) the maximum value of no-ops to run\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = env.n_actions[0]*env.n_actions[1] -1\n",
    "        print(\"Noop action N = \", self.noop_action)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = random.randint(1, self.noop_max + 1)\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"\n",
    "        Return only every `skip`-th frame (frameskipping)\n",
    "        :param env: (Gym Environment) the environment\n",
    "        :param skip: (int) number of `skip`-th frame\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=env.observation_space.dtype)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Step the environment with the given action\n",
    "        Repeat action, sum reward, and max over last observations.\n",
    "        :param action: ([int] or [float]) the action\n",
    "        :return: ([int] or [float], [float], [bool], dict) observation, reward, done, information\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        clips the reward to {+1, 0, -1} by its sign.\n",
    "        :param env: (Gym Environment) the environment\n",
    "        \"\"\"\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"\n",
    "        Bin reward to {+1, 0, -1} by its sign.\n",
    "        :param reward: (float)\n",
    "        \"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, hw_obs_resize = [84, 84]):\n",
    "        \"\"\"\n",
    "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
    "        :param env: (Gym Environment) the environment\n",
    "        \"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = hw_obs_resize[1]\n",
    "        self.height = hw_obs_resize[0]\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1),\n",
    "                                            dtype=env.observation_space.dtype)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        \"\"\"\n",
    "        returns the current observation from a frame\n",
    "        :param frame: ([int] or [float]) environment frame\n",
    "        :return: ([int] or [float]) the observation\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, n_frames):\n",
    "        \"\"\"Stack n_frames last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        stable_baselines.common.atari_wrappers.LazyFrames\n",
    "        :param env: (Gym Environment) the environment\n",
    "        :param n_frames: (int) the number of frames to stack\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n_frames = n_frames\n",
    "        self.frames = deque([], maxlen=n_frames)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * n_frames),\n",
    "                                            dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        for _ in range(self.n_frames):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.n_frames\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = spaces.Box(low=0, high=1.0, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"\n",
    "        This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to np.ndarray before being passed to the model.\n",
    "        :param frames: ([int] or [float]) environment frames\n",
    "        \"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=2)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n",
    "\n",
    "\n",
    "def make_diambra(diambraGame, diambra_kwargs):\n",
    "    \"\"\"\n",
    "    Create a wrapped atari Environment\n",
    "    :param env_id: (str) the environment ID\n",
    "    :return: (Gym Environment) the wrapped atari environment\n",
    "    \"\"\"\n",
    "    env = diambraGame(**diambra_kwargs)\n",
    "    env = NoopResetEnv(env, noop_max=6)\n",
    "    #env = MaxAndSkipEnv(env, skip=4)\n",
    "    return env\n",
    "\n",
    "\n",
    "def wrap_deepmind(env, clip_rewards=True, frame_stack=1, scale=False):\n",
    "    \"\"\"\n",
    "    Configure environment for DeepMind-style Atari.\n",
    "    :param env: (Gym Environment) the atari environment\n",
    "    :param clip_rewards: (bool) wrap the reward clipping wrapper\n",
    "    :param frame_stack: (bool) wrap the frame stacking wrapper\n",
    "    :param scale: (bool) wrap the scaling observation wrapper\n",
    "    :return: (Gym Environment) the wrapped atari environment\n",
    "    \"\"\"\n",
    "\n",
    "    # Resizing observation from H x W x 3 to hw_obs_resize[0] x hw_obs_resize[1] x 1\n",
    "    env = WarpFrame(env, hw_obs_resize = [84, 84])\n",
    "    \n",
    "    # Scales observations normalizing them between 0.0 and 1.0\n",
    "    if scale:\n",
    "        env = ScaledFloatFrame(env)\n",
    "        \n",
    "    # Clip rewards using sign function\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "        \n",
    "    # Stack #frame_stack frames together\n",
    "    if frame_stack > 1:\n",
    "        env = FrameStack(env, frame_stack)\n",
    "        \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_diambra_env(env_id, num_env, seed, wrapper_kwargs=None,\n",
    "                   start_index=0, allow_early_resets=True,\n",
    "                   start_method=None, use_subprocess=False):\n",
    "    \"\"\"\n",
    "    Create a wrapped, monitored VecEnv for Atari.\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "    :param seed: (int) the initial seed for RNG\n",
    "    :param wrapper_kwargs: (dict) the parameters for wrap_deepmind function\n",
    "    :param start_index: (int) start rank index\n",
    "    :param allow_early_resets: (bool) allows early reset of the environment\n",
    "    :param start_method: (str) method used to start the subprocesses.\n",
    "        See SubprocVecEnv doc for more information\n",
    "    :param use_subprocess: (bool) Whether to use `SubprocVecEnv` or `DummyVecEnv` when\n",
    "        `num_env` > 1, `DummyVecEnv` is usually faster. Default: False\n",
    "    :return: (VecEnv) The atari environment\n",
    "    \"\"\"\n",
    "    if wrapper_kwargs is None:\n",
    "        wrapper_kwargs = {}\n",
    "\n",
    "    def make_env(rank):\n",
    "        def _thunk():\n",
    "            env = make_diambra(env_id)\n",
    "            env.seed(seed + rank)\n",
    "            env = Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)),\n",
    "                          allow_early_resets=allow_early_resets)\n",
    "            return wrap_deepmind(env, **wrapper_kwargs)\n",
    "        return _thunk\n",
    "    set_global_seeds(seed)\n",
    "\n",
    "    # When using one environment, no need to start subprocesses\n",
    "    if num_env == 1 or not use_subprocess:\n",
    "        return DummyVecEnv([make_env(i + start_index) for i in range(num_env)])\n",
    "\n",
    "    return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)],\n",
    "                         start_method=start_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diambraKwargs = {}\n",
    "diambraKwargs[\"roms_path\"] = \"../../roms/MAMEToolkit/roms/\"\n",
    "diambraKwargs[\"binary_path\"] = \"../../../customMAME/\"\n",
    "diambraKwargs[\"frame_ratio\"] = 1\n",
    "diambraKwargs[\"player_id\"] = \"P1\"\n",
    "diambraKwargs[\"character\"] = \"Random\"\n",
    "\n",
    "env = make_diambra(diambraMame, diambra_kwargs = diambraKwargs)\n",
    "env = Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(0)),\n",
    "              allow_early_resets=True)\n",
    "env = wrap_deepmind(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "while True:\n",
    "\n",
    "    action = random.randint(0, 35)\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    print(\"Frames shape:\", observation.shape)\n",
    "    print(\"Reward:\", reward)\n",
    "    #print(\"Fighting = \", info[\"fighting\"])\n",
    "    #print(\"Rewards = \", info[\"rewards\"])\n",
    "    #print(\"HealthP1 = \", info[\"healthP1\"])\n",
    "    #print(\"HealthP2 = \", info[\"healthP2\"])\n",
    "    #print(\"PositionP1 = \", info[\"positionP1\"])\n",
    "    #print(\"PositionP2 = \", info[\"positionP2\"])\n",
    "    #print(\"WinP1 = \", info[\"winsP1\"])\n",
    "    #print(\"WinP2 = \", info[\"winsP2\"])\n",
    "    print(\"Done = \", done)\n",
    "\n",
    "    if done:\n",
    "        print(\"Resetting Env\")\n",
    "        observation = env.reset()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There already exists an environment generator\n",
    "# that will make and wrap atari environments correctly.\n",
    "# Here we are also multiprocessing training (num_env=4 => 4 processes)\n",
    "env = make_diambra_env('PongNoFrameskip-v4', num_env=4, seed=0,  wrapper_kwargs = {\"frame_stack\": True})\n",
    "\n",
    "# OR \n",
    "#env = make_atari_env('PongNoFrameskip-v4', num_env=1, seed=0)\n",
    "# Frame-stacking with 4 frames\n",
    "#env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "model = PPO2(CnnPolicy, env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "time_steps = 1000\n",
    "model.learn(total_timesteps=time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enjoy trained agent\n",
    "eval_env = make_atari_env('PongNoFrameskip-v4', num_env=1, seed=0,  wrapper_kwargs = {\"frame_stack\": True})\n",
    "obs = eval_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, info = eval_env.step(action)\n",
    "    eval_env.render()\n",
    "    if done:\n",
    "        obs = eval_env.reset()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
