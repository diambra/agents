{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from os.path import expanduser \n",
    "from os import listdir\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "import pickle, bz2 \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "homeDir = expanduser(\"~\") \n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../../utils'))\n",
    "\n",
    "from diambraImitationLearning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Show files in folder\n",
    "trajRecFolder = os.path.join(homeDir, \"DIAMBRA/trajRecordings/doapp\")\n",
    "trajectoriesFiles = [os.path.join(trajRecFolder, f) for f in listdir(trajRecFolder) if os.path.isfile(os.path.join(trajRecFolder, f))]\n",
    "print(trajectoriesFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diambraILKwargs = {}\n",
    "diambraILKwargs[\"hwcDim\"] = [256,256,5]\n",
    "diambraILKwargs[\"actionSpace\"] = \"discrete\" # or \"discrete\"\n",
    "diambraILKwargs[\"nActions\"] = [9, 8]\n",
    "diambraILKwargs[\"trajFilesList\"] = trajectoriesFiles# P1\n",
    "diambraILKwargs[\"totalCpus\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = makeDiambraImitationLearningEnv(diambraImitationLearning, diambraILKwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()[0]\n",
    "env.render(mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env_method(\"trajSummary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nChars = env.get_attr(\"nChars\")[0]\n",
    "charNames = env.get_attr(\"charNames\")[0]\n",
    "n_actions = env.get_attr(\"nActions\")[0]\n",
    "actBufLen = env.get_attr(\"actBufLen\")[0]\n",
    "playerId = env.get_attr(\"playerId\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limAct = [None, None]\n",
    "for idx in range(2):\n",
    "    limAct[idx] = [actBufLen * n_actions[0], \n",
    "                   actBufLen * n_actions[0] + actBufLen * n_actions[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Visualize Obs content\n",
    "def observationViz(observation, limAct):\n",
    "    \n",
    "    shp = observation.shape\n",
    "    additionalPar = int(observation[0,0,shp[2]-1])\n",
    "        \n",
    "    # 1P        \n",
    "    nScalarAddPar = additionalPar - nChars\\\n",
    "                    - actBufLen*(n_actions[0]+n_actions[1])\n",
    "\n",
    "    print(\"Additional Par = \", additionalPar)\n",
    "    print(\"N scalar actions = \", nScalarAddPar)\n",
    "    \n",
    "    addPar = observation[:,:,shp[2]-1]\n",
    "    addPar = np.reshape(addPar, (-1))\n",
    "    addPar = addPar[1:additionalPar+1]\n",
    "    actions = addPar[0:additionalPar-nScalarAddPar-nChars]\n",
    "        \n",
    "    moveActionsP1   = actions[0:limAct[0][0]]\n",
    "    attackActionsP1 = actions[limAct[0][0]:limAct[0][1]]\n",
    "    moveActionsP1   = np.reshape(moveActionsP1, (actBufLen,-1))\n",
    "    attackActionsP1 = np.reshape(attackActionsP1, (actBufLen,-1))\n",
    "    print(\"Move actions P1 =\\n\", moveActionsP1)\n",
    "    print(\"Attack actions P1 =\\n \", attackActionsP1)\n",
    "        \n",
    "    others = addPar[additionalPar-nScalarAddPar-nChars:]\n",
    "    print(\"ownHealth = \", others[0])\n",
    "    print(\"oppHealth = \", others[1])\n",
    "    print(\"ownPosition = \", others[2])\n",
    "    print(\"oppPosition = \", others[3])\n",
    "    print(\"stage = \", others[4])\n",
    "    print(\"Playing Char  = \", charNames[list(others[nScalarAddPar:\n",
    "                                                    nScalarAddPar + nChars]).index(1.0)])\n",
    "        \n",
    "    #input(\"Pause1\")\n",
    "        \n",
    "    obs = np.array(observation).astype(np.float32)\n",
    "    \n",
    "    for idx in range(shp[2]-1):\n",
    "        cv2.imshow(\"image\"+str(idx), obs[:,:,idx])\n",
    "    \n",
    "    cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulativeEpRew = 0.0\n",
    "cumulativeEpRewAll = []\n",
    "\n",
    "maxNumEp = 10\n",
    "currNumEp = 0\n",
    "\n",
    "procIdx = 0\n",
    "\n",
    "while currNumEp < maxNumEp:\n",
    "       \n",
    "    dummyActions = [0 for i in range(diambraILKwargs[\"totalCpus\"])]\n",
    "    observation, reward, done, info = env.step(dummyActions)\n",
    "    env.render(mode=\"human\")\n",
    "    \n",
    "    observation = observation[procIdx]\n",
    "    reward = reward[procIdx]\n",
    "    done = done[procIdx]\n",
    "    action = info[procIdx][\"action\"]\n",
    "    print(\"Reward = \", reward)\n",
    "    if done:\n",
    "        observation = info[procIdx][\"terminal_observation\"]\n",
    "    \n",
    "    # Visualize observations content\n",
    "    observationViz(observation, limAct) # Keep space bar pressed to continue env execution\n",
    " \n",
    "    cumulativeEpRew += reward\n",
    "    \n",
    "    if done:\n",
    "        currNumEp += 1\n",
    "        print(\"Ep. # = \", currNumEp)\n",
    "        print(\"Ep. Cumulative Rew # = \", cumulativeEpRew)\n",
    "        \n",
    "        cumulativeEpRewAll.append(cumulativeEpRew)\n",
    "        cumulativeEpRew = 0.0\n",
    "\n",
    "    if np.any(env.get_attr(\"exhausted\")):\n",
    "        break\n",
    "        \n",
    "print(\"All ep. rewards =\", cumulativeEpRewAll)   \n",
    "print(\"Mean cumulative reward =\", np.mean(cumulativeEpRewAll))    \n",
    "print(\"Std cumulative reward =\", np.std(cumulativeEpRewAll))       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
