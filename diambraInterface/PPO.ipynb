{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameFolder = \"DOA++-MAME\"\n",
    "#gameFolder = \"SFIII-MAME\"\n",
    "#gameFolder = \"UMK3-MAME\"\n",
    "#gameFolder = \"TEKTAG-MAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import time\n",
    "timeDepSeed = int((time.time()-int(time.time()-0.5))*1000)\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../../games',gameFolder))   \n",
    "\n",
    "tensorBoardFolder = \"./ppo2_TB_\" + gameFolder + \"/\"\n",
    "modelFolder = \"ppo2_Model_\" + gameFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from makeDiambraEnv import *\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from stable_baselines.common.policies import CnnPolicy\n",
    "from stable_baselines import PPO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env_id =  Train0\n"
     ]
    }
   ],
   "source": [
    "diambraKwargs = {}\n",
    "diambraKwargs[\"roms_path\"] = \"../../roms/MAMEToolkit/roms/\"\n",
    "diambraKwargs[\"binary_path\"] = \"../../../customMAME/\"\n",
    "diambraKwargs[\"player\"] = \"P1\"\n",
    "diambraKwargs[\"frame_ratio\"] = 3\n",
    "#diambraKwargs[\"render\"] =True\n",
    "#diambraKwargs[\"throttle\"] = False\n",
    "#diambraKwargs[\"sound\"] = False \n",
    "#diambraKwargs[\"character\"] =\"Random\"\n",
    "diambraKwargs[\"character\"] = \"Kasumi\"\n",
    "\n",
    "wrapperKwargs = {\"frame_stack\": True}\n",
    "wrapperKwargs[\"clip_rewards\"] = False\n",
    "wrapperKwargs[\"normalize_rewards\"] = True\n",
    "wrapperKwargs[\"scale\"] = True\n",
    "wrapperKwargs[\"hw_obs_resize\"] = [128, 128]\n",
    "\n",
    "numEnv=4\n",
    "\n",
    "env = make_diambra_env(diambraMame, env_prefix=\"Train\", num_env=numEnv, seed=timeDepSeed, \n",
    "                       diambra_kwargs = diambraKwargs, wrapper_kwargs = wrapperKwargs)\n",
    "\n",
    "# OR \n",
    "#env = make_diambra_env(diambraMame, num_env=2, seed=0, diambra_kwargs = diambraKwargs)\n",
    "# Frame-stacking with 4 frames\n",
    "#env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2(CnnPolicy, env, verbose=1, tensorboard_log=tensorBoardFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "#model = PPO2.load(modelFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "time_steps = 10000000\n",
    "model.learn(total_timesteps=time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(modelFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./tensorBoardFolder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "while True:\n",
    "\n",
    "    actions = []\n",
    "    for idx in range(numEnv):\n",
    "        actions.append(env.action_space.sample())\n",
    "\n",
    "    observation, reward, done, info = env.step(actions)\n",
    "\n",
    "    print(\"Frames shape:\", observation.shape)\n",
    "    print(\"Reward:\", reward)\n",
    "    #print(\"Fighting = \", info[\"fighting\"])\n",
    "    #print(\"Rewards = \", info[\"rewards\"])\n",
    "    #print(\"HealthP1 = \", info[\"healthP1\"])\n",
    "    #print(\"HealthP2 = \", info[\"healthP2\"])\n",
    "    #print(\"PositionP1 = \", info[\"positionP1\"])\n",
    "    #print(\"PositionP2 = \", info[\"positionP2\"])\n",
    "    #print(\"WinP1 = \", info[\"winsP1\"])\n",
    "    #print(\"WinP2 = \", info[\"winsP2\"])\n",
    "    print(\"Done = \", done)\n",
    "\n",
    "    if np.any(done):\n",
    "        print(\"Resetting Env\")\n",
    "        observation = env.reset()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enjoy trained agent\n",
    "eval_env = make_atari_env('PongNoFrameskip-v4', num_env=1, seed=0,  wrapper_kwargs = {\"frame_stack\": True})\n",
    "obs = eval_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, info = eval_env.step(action)\n",
    "    eval_env.render()\n",
    "    if done:\n",
    "        obs = eval_env.reset()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
